{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:33.534752Z",
     "start_time": "2019-03-12T14:57:33.084770Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import DutchStemmer, FrenchStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:33.673811Z",
     "start_time": "2019-03-12T14:57:33.667949Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, data_path='/Users/guillaumecorda/Desktop/UvA/Information Retrieval/project/data/', url=None):\n",
    "        self.data_path = data_path\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:33.752221Z",
     "start_time": "2019-03-12T14:57:33.749444Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:34.217648Z",
     "start_time": "2019-03-12T14:57:33.925233Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.data_path + 'final_data.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:34.861419Z",
     "start_time": "2019-03-12T14:57:34.219918Z"
    }
   },
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open(cfg.data_path+'big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:57:34.870228Z",
     "start_time": "2019-03-12T14:57:34.864092Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:58:04.518881Z",
     "start_time": "2019-03-12T14:58:04.503164Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_query():\n",
    "    \n",
    "    query = input('What are you looking for ? \\n' )\n",
    "    print()\n",
    "    lang = get_language(query)\n",
    "    \n",
    "    if lang !='en':\n",
    "        translator = Translator()\n",
    "        txt = translator.translate(query)\n",
    "        query = txt.text\n",
    "        print(query)\n",
    "        answer = input('Is it what you meant ? \\nyes/no \\n')\n",
    "        \n",
    "        if answer.lower() == 'no':\n",
    "            print('Please use english only')\n",
    "            sys.exit()\n",
    "            \n",
    "    #tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_sent = tokenizer.tokenize(query)\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    cleaned_txt = []\n",
    "    for w in tokenized_sent:\n",
    "        if w not in stop_words:\n",
    "            cleaned_txt.append(w)\n",
    "    # Spell checker\n",
    "    cleaned_txt = [correction(word) for word in cleaned_txt]\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words=[]\n",
    "    for w in cleaned_txt:\n",
    "        word = ps.stem(w)\n",
    "        if word=='apart':\n",
    "            word = word.replace('apart', 'apartment')\n",
    "        stemmed_words.append(word) \n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T14:58:07.049444Z",
     "start_time": "2019-03-12T14:58:05.079245Z"
    }
   },
   "outputs": [],
   "source": [
    "# example_dutch ---> Ik ben op zoek naar een appartement in Amsterdam met een keuken en twee slaapkamers\n",
    "# example_brazil ---> Eu estou procurando um apartamento em Amsterdam com uma cozinha e dois quartos\n",
    "\n",
    "#run this line without any parameter then enter one of the three sentences above in the text field to test function\n",
    "query = make_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
