{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:14:23.468361Z",
     "start_time": "2019-03-12T13:14:18.385017Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import ast\n",
    "from multiprocessing import Pool, Process\n",
    "\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import DutchStemmer, FrenchStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:14:23.483073Z",
     "start_time": "2019-03-12T13:14:23.473643Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, data_path='/Users/guillaumecorda/Desktop/UvA/Information Retrieval/project/data/', url=None):\n",
    "        self.data_path = data_path\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:14:23.491495Z",
     "start_time": "2019-03-12T13:14:23.485413Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:40:52.723614Z",
     "start_time": "2019-03-06T17:40:52.588430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ams = pd.read_csv('data/ams_data.csv', encoding='utf-8', engine='python')\n",
    "df_rot = pd.read_csv('data/rot_data.csv', encoding='utf-8', engine='python')\n",
    "df_haag = pd.read_csv('data/haag_data.csv', encoding='utf-8', engine='python')\n",
    "df_gro = pd.read_csv('data/gro_data.csv', encoding='utf-8', engine='python')\n",
    "df_utr = pd.read_csv('data/utr_data.csv', encoding='utf-8', engine='python')\n",
    "df_ein = pd.read_csv('data/ein_data.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:42:48.139924Z",
     "start_time": "2019-03-06T17:42:48.057320Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.data_path + 'crawled_data.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:56:30.603712Z",
     "start_time": "2019-03-07T16:56:30.215607Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(cfg.data_path + 'data_merged.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:56:55.033513Z",
     "start_time": "2019-03-07T16:56:55.024336Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.drop(['zipcode', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:56:58.264769Z",
     "start_time": "2019-03-07T16:56:58.180770Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:42:50.623449Z",
     "start_time": "2019-03-06T17:42:50.610262Z"
    }
   },
   "outputs": [],
   "source": [
    "df['url'].to_csv(cfg.data_path + 'urls.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:39:52.692609Z",
     "start_time": "2019-03-06T17:39:52.677259Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_correct_address(df):\n",
    "    ind_list = list()\n",
    "    for i in range(df.shape[0]):\n",
    "        if 'renting' in df['Location'].iloc[i]:\n",
    "            tmp = df['Location'].iloc[i].split()\n",
    "            df['Location'].iloc[i] = ' '.join(word for word in tmp[-4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:42:54.304922Z",
     "start_time": "2019-03-06T17:42:54.294883Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_city(row):\n",
    "    try:\n",
    "        if 'Amsterdam' in row:\n",
    "            return 'Amsterdam'\n",
    "        elif 'The Hague' in row:\n",
    "            return 'The Hague'\n",
    "        elif 'Rotterdam' in row:\n",
    "            return 'Rotterdam'\n",
    "        elif 'Utrecht' in row:\n",
    "            return 'Utrecht'\n",
    "        elif 'Groningen' in row:\n",
    "            return 'Groningen'\n",
    "        elif 'Eindhoven' in row:\n",
    "            return 'Eindhoven'\n",
    "        elif 'Den Haag' in row:\n",
    "            return 'The Hague'\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:42:54.792560Z",
     "start_time": "2019-03-06T17:42:54.783194Z"
    }
   },
   "outputs": [],
   "source": [
    "df['City'] = df['Location'].map(get_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:42:54.878297Z",
     "start_time": "2019-03-06T17:42:54.866188Z"
    }
   },
   "outputs": [],
   "source": [
    "df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Translate Dutch to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:17:15.362900Z",
     "start_time": "2019-03-12T13:17:15.358373Z"
    }
   },
   "outputs": [],
   "source": [
    "def txt_translator(row):\n",
    "    try:\n",
    "        translator = Translator()\n",
    "        new_text = translator.translate(row).text\n",
    "        return new_text\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:17:19.197029Z",
     "start_time": "2019-03-12T13:17:19.186977Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:09:09.987295Z",
     "start_time": "2019-03-08T12:09:09.971414Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_analysis(row):\n",
    "    \n",
    "    try:\n",
    "        text = row['Description']\n",
    "        #tokenization\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokenized_sent = tokenizer.tokenize(text)\n",
    "\n",
    "        #stopwords removal \n",
    "        if row['Language']=='nl':\n",
    "            stop_words = set(stopwords.words(\"dutch\"))\n",
    "            \n",
    "            cleaned_txt = []\n",
    "            for w in tokenized_sent:\n",
    "                if w not in stop_words:\n",
    "                    cleaned_txt.append(w)\n",
    "\n",
    "            # Stemming\n",
    "            ps = DutchStemmer()\n",
    "            stemmed_words=[]\n",
    "            for w in cleaned_txt:\n",
    "                stemmed_words.append(ps.stem(w))\n",
    "                \n",
    "        elif row['Language']=='fr':\n",
    "            stop_words = set(stopwords.words(\"french\"))\n",
    "            \n",
    "            cleaned_txt = []\n",
    "            for w in tokenized_sent:\n",
    "                if w not in stop_words:\n",
    "                    cleaned_txt.append(w)\n",
    "            # Stemming\n",
    "            ps = FrenchStemmer()\n",
    "            stemmed_words=[]\n",
    "            for w in cleaned_txt:\n",
    "                stemmed_words.append(ps.stem(w))\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "            cleaned_txt = []\n",
    "            for w in tokenized_sent:\n",
    "                if w not in stop_words:\n",
    "                    cleaned_txt.append(w)\n",
    "            # Stemming\n",
    "            ps = PorterStemmer()\n",
    "            stemmed_words=[]\n",
    "            for w in cleaned_txt:\n",
    "                stemmed_words.append(ps.stem(w))\n",
    "\n",
    "            stemmed_words = [w.replace('apart', 'apartment') for w in stemmed_words]\n",
    "        \n",
    "        return stemmed_words\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guillaume's data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:58:09.831892Z",
     "start_time": "2019-03-07T16:57:21.218433Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all['Language'] = df_all['Description'].map(get_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:28:58.834831Z",
     "start_time": "2019-03-07T16:28:12.766703Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Language'] = df['Description'].map(get_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:43:08.109134Z",
     "start_time": "2019-03-06T17:43:07.945202Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Language'].loc[df['Language']=='af'] = 'nl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:43:08.130823Z",
     "start_time": "2019-03-06T17:43:08.116509Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:43:11.886947Z",
     "start_time": "2019-03-06T17:43:08.136111Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Stem'] = df.apply(text_analysis, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:30:04.448197Z",
     "start_time": "2019-03-07T16:30:04.438770Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['zipcode', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:43:27.532329Z",
     "start_time": "2019-03-06T17:43:27.385490Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(cfg.data_path + 'data_processed.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:58:09.893267Z",
     "start_time": "2019-03-07T16:58:09.834451Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:59:34.893787Z",
     "start_time": "2019-03-07T16:59:34.550760Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.to_csv(cfg.data_path + 'final_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Index without ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T17:45:32.446411Z",
     "start_time": "2019-03-06T17:45:32.340601Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.data_path + 'data_processed.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:22.505963Z",
     "start_time": "2019-03-07T17:33:22.187726Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(cfg.data_path + 'final_data.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:24.973380Z",
     "start_time": "2019-03-07T17:33:24.961932Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:24.009859Z",
     "start_time": "2019-03-07T17:33:23.363056Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final['Language'].loc[df_final['Language']=='af'] = 'nl'\n",
    "df_final['Language'].loc[df_final['Language']=='cy'] = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:09:14.466795Z",
     "start_time": "2019-03-08T12:09:14.461077Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_description(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:09:14.617531Z",
     "start_time": "2019-03-08T12:09:14.609786Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(df, lang):\n",
    "\n",
    "    index_en = df['Stem'].loc[df['Language']==lang].index\n",
    "    vocabulary = df['Stem'].loc[df['Language']==lang].loc[index_en[0]]\n",
    "\n",
    "    for i in index_en[1:]:\n",
    "        if df['Stem'].loc[df['Language']==lang].loc[i] is not None :\n",
    "            vocabulary += df['Stem'].loc[df['Language']==lang].loc[i]\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:34.417447Z",
     "start_time": "2019-03-07T17:33:33.072222Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final['Stem'] = df_final['Stem'].map(format_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:34.548878Z",
     "start_time": "2019-03-07T17:33:34.420995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:41.869886Z",
     "start_time": "2019-03-07T17:33:35.085003Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_en = set(get_vocabulary(df_final, 'en'))\n",
    "vocab_nl = set(get_vocabulary(df_final, 'nl'))\n",
    "vocab_fr = set(get_vocabulary(df_final, 'fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:33:41.877398Z",
     "start_time": "2019-03-07T17:33:41.872596Z"
    }
   },
   "outputs": [],
   "source": [
    "len(vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:09:17.033532Z",
     "start_time": "2019-03-08T12:09:17.024592Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_word_in_description(df, vocab, lang):\n",
    "    \n",
    "    distributed_index = dict.fromkeys(vocab)\n",
    "    try:\n",
    "        for i in df.loc[df['Language']==lang].index:\n",
    "            row = df['Stem'].loc[i]\n",
    "            for word in vocab:\n",
    "                if word in row:\n",
    "                    if distributed_index[word] is None:\n",
    "                        distributed_index[word] = [[i, row.index(word)]]\n",
    "                    else:\n",
    "                        distributed_index[word].append([i, row.index(word)])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return distributed_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T10:48:32.840020Z",
     "start_time": "2019-02-22T10:48:11.441539Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for i in range(5):\n",
    "    p = Process(target=is_word_in_description, args=(vocab_en, 'en'))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "\n",
    "for proc in jobs:\n",
    "    proc.join()\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:37:00.176053Z",
     "start_time": "2019-03-07T17:33:41.889785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distributed_index_en = is_word_in_description(df_final, vocab_en, 'en')\n",
    "distributed_index_nl = is_word_in_description(df_final, vocab_nl, 'nl')\n",
    "distributed_index_fr = is_word_in_description(df_final, vocab_fr, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:37:00.823767Z",
     "start_time": "2019-03-07T17:37:00.178378Z"
    }
   },
   "outputs": [],
   "source": [
    "distributed_index_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:37:00.856403Z",
     "start_time": "2019-03-07T17:37:00.826228Z"
    }
   },
   "outputs": [],
   "source": [
    "distributed_index = {'english' : distributed_index_en, 'dutch' : distributed_index_nl,\n",
    "                     'french' : distributed_index_fr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T17:37:24.623455Z",
     "start_time": "2019-03-07T17:37:24.212724Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('distributed_index_en.npy', distributed_index_en)\n",
    "np.save('distributed_index_nl.npy', distributed_index_nl)\n",
    "np.save('distributed_index_fr.npy', distributed_index_fr)\n",
    "np.save('distributed_index.npy', distributed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T13:05:24.513632Z",
     "start_time": "2019-03-08T13:05:22.552298Z"
    }
   },
   "outputs": [],
   "source": [
    "distributed_index_en = np.load('distributed_index_en.npy').item()\n",
    "distributed_index_nl = np.load('distributed_index_nl.npy').item()\n",
    "distributed_index_fr = np.load('distributed_index_fr.npy').item()\n",
    "distributed_index = np.load('distributed_index.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:14:28.219012Z",
     "start_time": "2019-03-12T13:14:27.859507Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.data_path + 'final_data.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:14:28.317816Z",
     "start_time": "2019-03-12T13:14:28.224008Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply text analysis on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:25:48.236420Z",
     "start_time": "2019-03-12T13:25:48.222820Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_query():\n",
    "    \n",
    "    query = input('What are you looking for ? \\n' )\n",
    "    lang = get_language(query)\n",
    "    \n",
    "    #tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_sent = tokenizer.tokenize(query)\n",
    "    \n",
    "    \n",
    "    #stopwords removal \n",
    "    if lang=='nl':\n",
    "        stop_words = set(stopwords.words(\"dutch\"))\n",
    "\n",
    "        cleaned_txt = []\n",
    "        for w in tokenized_sent:\n",
    "            if w not in stop_words:\n",
    "                cleaned_txt.append(w)\n",
    "\n",
    "        # Stemming\n",
    "        ps = DutchStemmer()\n",
    "        stemmed_words=[]\n",
    "        for w in cleaned_txt:\n",
    "            stemmed_words.append(ps.stem(w))\n",
    "\n",
    "    elif lang=='fr':\n",
    "        stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "        cleaned_txt = []\n",
    "        for w in tokenized_sent:\n",
    "            if w not in stop_words:\n",
    "                cleaned_txt.append(w)\n",
    "        # Stemming\n",
    "        ps = FrenchStemmer()\n",
    "        stemmed_words=[]\n",
    "        for w in cleaned_txt:\n",
    "            stemmed_words.append(ps.stem(w))\n",
    "\n",
    "\n",
    "    else:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "        cleaned_txt = []\n",
    "        for w in tokenized_sent:\n",
    "            if w not in stop_words:\n",
    "                cleaned_txt.append(w)\n",
    "        # Stemming\n",
    "        ps = PorterStemmer()\n",
    "        stemmed_words=[]\n",
    "        for w in cleaned_txt:\n",
    "            word = ps.stem(w)\n",
    "            if w=='apart':\n",
    "                word = w.replace('apart', 'apartment')\n",
    "            stemmed_words.append(ps.stem(w)) \n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:26:04.463285Z",
     "start_time": "2019-03-12T13:25:53.646757Z"
    }
   },
   "outputs": [],
   "source": [
    "query = create_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T13:26:14.929420Z",
     "start_time": "2019-03-12T13:26:14.923265Z"
    }
   },
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:58:47.807057Z",
     "start_time": "2019-03-08T14:58:47.798021Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_data_before_matching(df, query):\n",
    "    df = pd.DataFrame(columns=['Available From', 'City', 'Interior', 'Location', 'Number of bedrooms', 'Price',\n",
    "        'Rooms', 'Surface min', 'Surface max'])\n",
    "    col = df.columns[0]\n",
    "    df_tmp = df.loc[df[col] == query[col].iloc[0]]\n",
    "    print(df_tmp)\n",
    "    for col in df.columns[1:]:\n",
    "        print(col)\n",
    "        if query[col].iloc[0] != -1:\n",
    "            print(df.loc[df[col] == query[col].iloc[0]])\n",
    "            df_tmp = pd.concat(df_tmp, df.loc[df[col] == query[col].iloc[0]])\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:58:48.372110Z",
     "start_time": "2019-03-08T14:58:48.332090Z"
    }
   },
   "outputs": [],
   "source": [
    "filter_data_before_matching(df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:45:52.932095Z",
     "start_time": "2019-03-08T14:45:52.803531Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if query['City'] is not None:\n",
    "    df_tmp = df.loc[query['City'].iloc[0] in df['City']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:46:22.031602Z",
     "start_time": "2019-03-08T14:46:21.956114Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['City'] == 'Amsterdam' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best row using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:03:11.746590Z",
     "start_time": "2019-03-08T15:03:11.739544Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_matching_data(query, index):\n",
    "    rows = list()\n",
    "    for word in query :\n",
    "        try:\n",
    "            position = index[word]\n",
    "            print(position)\n",
    "            for el in position:\n",
    "                rows.append(el[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:03:12.190491Z",
     "start_time": "2019-03-08T15:03:12.183878Z"
    }
   },
   "outputs": [],
   "source": [
    "find_matching_data(['now', 'Amsterdam'], distributed_index_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:03:13.582199Z",
     "start_time": "2019-03-08T15:03:13.572429Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:12:56.236630Z",
     "start_time": "2019-03-08T15:12:56.233076Z"
    }
   },
   "outputs": [],
   "source": [
    "query_test = [\"Parkinggarage with private parking spot includedSplendid 3- room apartment of approx. 117m2 with large adjacent terrace facing South of approx. 60m2, overlooking the Sloterpark.This luxurious and practical apartment has 2 good size bedrooms, a walk-in closet, a luxurious bathroom with whirlpool, separate rainshower, and sink. The openplan kitchen is equipped with everyday modern appliances, such as a 5 spots induction hob, large oven, microwave combi, two door American refrigerator. There is a separate toilet with fontain.Furthermore the apartment has a spacious laundryroom with washer and dryer, large storage in the basement, and a private parkingspot.Above all this apartment offers a grand and bright livingroom with acces to the huge roofterrace facing South.The terrace has a dining and lounge area, from which you can enjoy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:12:59.254234Z",
     "start_time": "2019-03-08T15:12:59.247461Z"
    }
   },
   "outputs": [],
   "source": [
    "text_1 = [df['Description'].iloc[0]]\n",
    "text_2 = [df['Description'].iloc[282]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:13:07.555351Z",
     "start_time": "2019-03-08T15:13:07.541040Z"
    }
   },
   "outputs": [],
   "source": [
    "query_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:13:10.426794Z",
     "start_time": "2019-03-08T15:13:10.406297Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted_1 = tf.fit(text_1)\n",
    "txt_transformed_1 = txt_fitted.transform(text_1)\n",
    "\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted_2 = tf.fit(text_2)\n",
    "txt_transformed_2 = txt_fitted.transform(text_2)\n",
    "\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted_query = tf.fit(query_test)\n",
    "txt_transformed_query = txt_fitted.transform(query_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:13:38.007789Z",
     "start_time": "2019-03-08T15:13:37.994399Z"
    }
   },
   "outputs": [],
   "source": [
    "txt_fitted_1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:29.229340Z",
     "start_time": "2019-03-08T15:04:29.224604Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_1 = txt_transformed_1.todense()\n",
    "out_2 = txt_transformed_2.todense()\n",
    "out_query = txt_transformed_query.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:29.457616Z",
     "start_time": "2019-03-08T15:04:29.415919Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:30.172730Z",
     "start_time": "2019-03-08T15:04:30.165568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "output_1 = cosine_similarity(out_1, out_query)\n",
    "output_2 = cosine_similarity(out_2, out_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:30.362481Z",
     "start_time": "2019-03-08T15:04:30.350141Z"
    }
   },
   "outputs": [],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:30.751219Z",
     "start_time": "2019-03-08T15:04:30.742279Z"
    }
   },
   "outputs": [],
   "source": [
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:04:31.156592Z",
     "start_time": "2019-03-08T15:04:31.151725Z"
    }
   },
   "outputs": [],
   "source": [
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "646px",
    "left": "235px",
    "top": "111.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
